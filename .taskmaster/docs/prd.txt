# PDF to Vector Store Pipeline - Product Requirements Document

## Executive Summary

This document outlines the requirements for a Python-based system that processes uploaded PDF textbooks, converts them to structured text, semantically segments content by chapters, and stores vectorized chunks in Neo4j for intelligent retrieval in the LearnTrac educational platform.

## Project Overview

### Problem Statement
Educators need to upload textbook PDFs that can be intelligently parsed, chunked, and stored in a vector database for semantic search, content recommendation, and AI-powered learning experiences. Current manual content organization is time-intensive and lacks semantic understanding.

### Solution Overview
A comprehensive Python pipeline that:
1. Accepts PDF uploads through a web interface
2. Extracts and cleans text with intelligent spacing correction
3. Detects document structure (chapters, sections, subsections)
4. Filters content to exclude prefatory material and appendices
5. Semantically chunks content while preserving educational structure
6. Generates embeddings and stores in Neo4j vector database
7. Provides retrieval interface for educational applications

## Architecture Overview

### System Components

#### 1. PDF Upload Interface
- Web-based file upload component integrated into Trac wiki
- Support for PDF files up to 100MB
- Progress tracking and validation
- Metadata input (subject, course level, author)

#### 2. PDF Processing Engine
- **PDFProcessor**: Text extraction with spacing correction
- **StructureDetector**: Chapter/section identification using regex patterns
- **ContentFilter**: Removal of prefatory/appendix content
- **ContentChunker**: Intelligent segmentation preserving educational structure

#### 3. Embedding Pipeline
- **EmbeddingService**: Vector generation using OpenAI/sentence-transformers
- **BatchProcessor**: Efficient batch processing with rate limiting
- **QualityValidator**: Embedding validation and quality checks

#### 4. Neo4j Vector Store
- **VectorDatabase**: Neo4j integration with vector indexing
- **MetadataManager**: Rich metadata storage and relationships
- **QueryInterface**: Semantic search and retrieval capabilities

## Detailed Requirements

### 1. PDF Processing Requirements

#### 1.1 Text Extraction
- **Primary Library**: PyMuPDF (fitz) for high-quality text extraction
- **Fallback Options**: pdfplumber for complex layouts, pypdf for simple cases
- **Text Cleaning**: 
  - Correct missing spaces between words (common PDF extraction issue)
  - Normalize whitespace while preserving paragraph structure
  - Remove page numbers, headers, footers
  - Handle mathematical content and special characters
  - Preserve sentence boundaries and formatting cues

#### 1.2 Structure Detection
- **Chapter Detection Patterns**:
  ```python
  chapter_patterns = [
      r'^(?:Chapter|CHAPTER|Ch\.?)\s*(\d+)(?:\s*[:.\-]\s*(.+))?$',
      r'^(\d+)\s*[:.\-]\s*(.+)$',
      r'^Unit\s+(\d+)(?:\s*[:.\-]\s*(.+))?$',
      r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+Level\s+of\s+Organization)?)$'
  ]
  ```
- **Section Detection**:
  - Numbered sections (1.1, 1.2, etc.)
  - Descriptive headings with title case
  - Confidence scoring for structure quality
- **Validation**: Minimum 3 chapters detected for processing to continue

#### 1.3 Content Filtering
- **Prefatory Content Removal**:
  - Everything before first chapter or "Preface" section
  - Table of Contents, Copyright pages, Acknowledgments
  - Author biographies and institutional information
- **Appendix Removal**:
  - Content starting with "Appendix" section
  - Bibliography, Glossary, Index sections
  - Answer keys and supplementary materials

### 2. Chunking Strategy

#### 2.1 Hybrid Chunking Approach
Based on analysis of existing TypeScript implementation:

- **Content-Aware Chunking** (when structure detection quality > 0.3):
  - Respect chapter/section boundaries (never split across structural elements)
  - Target chunk size: 1000-1500 characters
  - Minimum chunk size: 300 characters
  - Overlap: 150 characters within same section only
  
- **Fallback Chunking** (poor structure detection):
  - Sentence-boundary splitting
  - Fixed-size chunks with word-boundary preservation
  - Standard overlap: 200 characters

#### 2.2 Educational Content Preservation
- **Mathematical Content**: Preserve formulas and equations as complete units
- **Definitions**: Keep definition + explanation together
- **Examples**: Maintain example problems with solutions
- **Exercise Sets**: Group related problems appropriately

#### 2.3 Chunk Metadata
```python
@dataclass
class ChunkMetadata:
    book_id: str
    title: str
    subject: str
    chapter: Optional[str]
    section: Optional[str]
    chunk_id: str
    chunk_index: int
    total_chunks: int
    content_type: Literal['text', 'formula', 'definition', 'example', 'exercise']
    difficulty: Optional[Literal['beginner', 'intermediate', 'advanced']]
    keywords: List[str]
    page_numbers: List[int]
    confidence_score: float
```

### 3. Embedding Generation

#### 3.1 Embedding Models
- **Primary**: OpenAI text-embedding-3-small (1536 dimensions) - cost-effective
- **Alternative**: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions) - local processing
- **Specialized**: Models fine-tuned for educational content if available

#### 3.2 Batch Processing
- **Batch Size**: 100 chunks per API request (OpenAI limit: 2048)
- **Rate Limiting**: 3 requests/second to avoid API limits
- **Error Handling**: Individual chunk retry on batch failures
- **Progress Tracking**: Real-time progress updates for UI

#### 3.3 Quality Assurance
- **Validation**: Ensure embeddings have correct dimensions
- **Similarity Checks**: Flag suspiciously similar chunks (potential duplicates)
- **Distribution Analysis**: Verify embedding diversity

### 4. Neo4j Integration

#### 4.1 Graph Schema
```cypher
// Textbook nodes
CREATE (t:Textbook {
    id: string,
    title: string,
    subject: string,
    author: string,
    upload_date: datetime,
    total_chunks: integer,
    processing_status: string
})

// Chapter nodes
CREATE (c:Chapter {
    id: string,
    title: string,
    number: string,
    textbook_id: string,
    chunk_count: integer
})

// Section nodes  
CREATE (s:Section {
    id: string,  
    title: string,
    number: string,
    chapter_id: string,
    level: integer
})

// Chunk nodes with vector embeddings
CREATE (ch:Chunk {
    id: string,
    text: string,
    content_type: string,
    keywords: [string],
    chunk_index: integer,
    embedding: [float]  // Vector embedding
})

// Relationships
CREATE (t)-[:HAS_CHAPTER]->(c)
CREATE (c)-[:HAS_SECTION]->(s)  
CREATE (s)-[:CONTAINS_CHUNK]->(ch)
CREATE (c)-[:CONTAINS_CHUNK]->(ch)
CREATE (t)-[:CONTAINS_CHUNK]->(ch)
```

#### 4.2 Vector Indexing
```cypher
// Create vector index for semantic search
CREATE VECTOR INDEX chunk_embeddings
FOR (c:Chunk) ON (c.embedding)
OPTIONS {indexConfig: {
    `vector.dimensions`: 1536,
    `vector.similarity_function`: 'cosine'
}}
```

#### 4.3 Query Capabilities
- **Semantic Search**: Find similar chunks by vector similarity
- **Structured Queries**: Search within specific chapters/sections  
- **Hybrid Search**: Combine semantic + keyword + structural filters
- **Relationship Traversal**: Navigate from chunks to chapters to textbooks

### 5. User Interface Requirements

#### 5.1 Upload Interface
- **File Selection**: Drag-and-drop PDF upload
- **Metadata Input**:
  - Subject/Course (dropdown with common options)
  - Difficulty Level (Beginner/Intermediate/Advanced)
  - Author (optional, extracted from PDF if available)
  - Description (optional)
- **Processing Options**:
  - Custom chapter pattern (for non-standard textbooks)
  - Embedding model selection
  - Chunk size preferences

#### 5.2 Progress Tracking
- **Real-time Updates**: WebSocket or SSE for live progress
- **Stage Indicators**:
  - PDF Upload (0-10%)
  - Text Extraction (10-30%)
  - Structure Detection (30-40%)
  - Content Filtering (40-50%)
  - Chunking (50-70%)
  - Embedding Generation (70-90%)
  - Database Storage (90-100%)
- **Error Reporting**: Clear error messages with retry options

#### 5.3 Management Dashboard
- **Textbook Library**: View all processed textbooks
- **Processing Status**: Monitor active/queued processing jobs
- **Quality Metrics**: Structure detection confidence, chunk statistics
- **Search Interface**: Test semantic search capabilities

### 6. Performance Requirements

#### 6.1 Processing Performance
- **Small PDF** (<5MB, <100 pages): Complete processing in <5 minutes
- **Medium PDF** (5-20MB, 100-500 pages): Complete processing in <20 minutes  
- **Large PDF** (20-100MB, 500+ pages): Complete processing in <60 minutes
- **Memory Usage**: Process 100MB PDFs with <4GB RAM

#### 6.2 Storage Efficiency
- **Chunk Size Optimization**: Balance retrieval quality vs. storage cost
- **Deduplication**: Identify and merge duplicate chunks across textbooks
- **Compression**: Consider embedding compression for storage optimization

#### 6.3 Query Performance
- **Semantic Search**: <500ms for similarity queries
- **Structured Queries**: <200ms for metadata-based searches
- **Batch Retrieval**: Handle 100+ concurrent chunk retrievals

### 7. Integration Requirements

#### 7.1 Trac Wiki Integration
- **Plugin Architecture**: Implement as Trac plugin using IRequestHandler
- **Authentication**: Use existing Trac authentication system
- **Permissions**: New permission level "TEXTBOOK_UPLOAD"
- **UI Integration**: Add to existing Trac navigation and styling

#### 7.2 API Endpoints
```python
# RESTful API for textbook management
POST   /api/textbooks/upload          # Upload and process PDF
GET    /api/textbooks/{id}            # Get textbook metadata
GET    /api/textbooks/{id}/chunks     # Get textbook chunks
POST   /api/textbooks/search          # Semantic search across textbooks
DELETE /api/textbooks/{id}            # Remove textbook and chunks
GET    /api/processing/{job_id}       # Check processing status
```

#### 7.3 Database Integration
- **Neo4j Connection**: Use existing Neo4j instance or create dedicated database
- **Connection Pooling**: Efficient connection management
- **Transaction Handling**: Atomic operations for data consistency
- **Backup Integration**: Include in existing backup procedures

### 8. Quality Assurance

#### 8.1 Content Quality Metrics
- **Structure Detection Confidence**: Minimum 70% for automated processing
- **Chunk Coherence**: Validate chunks contain complete thoughts/concepts
- **Embedding Quality**: Verify semantic similarity makes educational sense
- **Metadata Accuracy**: Cross-validate extracted chapter/section titles

#### 8.2 Testing Strategy
- **Unit Tests**: Individual component testing (PDF processing, chunking, embedding)
- **Integration Tests**: End-to-end pipeline testing with sample textbooks
- **Performance Tests**: Processing time and memory usage validation
- **Quality Tests**: Semantic search relevance evaluation

#### 8.3 Error Handling
- **Graceful Degradation**: Fallback strategies for each processing stage
- **Resume Capability**: Resume interrupted processing jobs
- **Validation Checkpoints**: Validate data quality at each stage
- **User Feedback**: Clear error messages with suggested corrections

### 9. Security Requirements

#### 9.1 Upload Security
- **File Validation**: Verify PDF format and scan for malicious content
- **Size Limits**: Enforce maximum file size (100MB)
- **Virus Scanning**: Integrate with antivirus scanning if available
- **Rate Limiting**: Prevent abuse of upload endpoints

#### 9.2 Data Protection
- **Access Control**: Respect Trac permissions for textbook access
- **Data Encryption**: Encrypt sensitive data at rest and in transit
- **Audit Logging**: Log all upload and processing activities
- **Privacy Compliance**: Handle educational content according to regulations

### 10. Monitoring and Maintenance

#### 10.1 Operational Monitoring
- **Processing Metrics**: Success rates, processing times, error frequencies
- **Resource Usage**: CPU, memory, disk space, API quota consumption
- **Quality Metrics**: Embedding generation success rates, search relevance
- **User Activity**: Upload frequency, search patterns, error reports

#### 10.2 Maintenance Procedures
- **Index Optimization**: Regular Neo4j index maintenance
- **Embedding Updates**: Procedures for reprocessing with new models
- **Data Cleanup**: Remove outdated or unused textbook data
- **Performance Tuning**: Optimize chunk sizes and embedding parameters

## Success Criteria

### Primary Success Metrics
1. **Processing Accuracy**: 95%+ successful PDF processing rate
2. **Search Relevance**: 85%+ user satisfaction with semantic search results  
3. **Performance**: Meet processing time requirements for all PDF sizes
4. **Reliability**: <1% data loss rate during processing
5. **User Adoption**: 80%+ of uploaded textbooks successfully integrated into learning workflows

### Secondary Success Metrics
1. **Cost Efficiency**: Processing cost <$0.10 per textbook
2. **Storage Optimization**: <50MB average storage per processed textbook
3. **Query Performance**: 99% of queries complete within performance targets
4. **System Stability**: 99.9% uptime for processing services
5. **Educational Impact**: Measurable improvement in content discoverability

## Implementation Timeline

### Phase 1: Core Processing Pipeline (4-6 weeks)
- PDF processing and text extraction
- Structure detection and content filtering  
- Basic chunking implementation
- Neo4j schema and basic storage

### Phase 2: Embedding and Search (3-4 weeks)
- Embedding generation with OpenAI integration
- Vector indexing in Neo4j
- Basic semantic search capabilities
- Quality validation and metrics

### Phase 3: User Interface (3-4 weeks)
- Trac plugin development
- Upload interface and progress tracking
- Management dashboard
- API endpoint implementation

### Phase 4: Optimization and Testing (2-3 weeks)
- Performance optimization
- Comprehensive testing
- Documentation and deployment
- User training and feedback collection

## Risk Mitigation

### Technical Risks
- **PDF Format Variations**: Maintain library of test PDFs covering edge cases
- **Structure Detection Failures**: Implement manual override capabilities
- **API Rate Limits**: Build in retry logic and alternative embedding models
- **Neo4j Performance**: Plan for indexing optimization and query tuning

### Operational Risks  
- **Large File Processing**: Implement chunked upload and processing queues
- **Concurrent Usage**: Design for multi-user concurrent processing
- **Storage Growth**: Plan for storage scaling and archival procedures
- **Integration Issues**: Thorough testing with existing Trac installation

This comprehensive PRD provides the foundation for implementing a robust PDF-to-vector pipeline that will significantly enhance LearnTrac's content management and semantic search capabilities.