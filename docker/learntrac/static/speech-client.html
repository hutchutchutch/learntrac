<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LearnTrac Speech Interface</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #333;
            text-align: center;
        }
        
        .controls {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 30px 0;
        }
        
        button {
            padding: 15px 30px;
            font-size: 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: all 0.3s;
        }
        
        .record-btn {
            background-color: #4CAF50;
            color: white;
        }
        
        .record-btn.recording {
            background-color: #f44336;
            animation: pulse 1s infinite;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
        
        .status {
            text-align: center;
            margin: 20px 0;
            font-size: 18px;
            color: #666;
        }
        
        .transcript {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            min-height: 100px;
            white-space: pre-wrap;
        }
        
        .response {
            background-color: #e3f2fd;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            min-height: 100px;
        }
        
        .audio-level {
            width: 100%;
            height: 30px;
            background-color: #ddd;
            border-radius: 15px;
            overflow: hidden;
            margin: 20px 0;
        }
        
        .audio-level-fill {
            height: 100%;
            background-color: #4CAF50;
            width: 0%;
            transition: width 0.1s;
        }
        
        .debug-link {
            text-align: center;
            margin-top: 30px;
        }
        
        .debug-link a {
            color: #2196F3;
            text-decoration: none;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ¤ LearnTrac Speech Interface</h1>
        
        <div class="status" id="status">Click "Start Recording" to begin</div>
        
        <div class="audio-level">
            <div class="audio-level-fill" id="audioLevel"></div>
        </div>
        
        <div class="controls">
            <button class="record-btn" id="recordBtn" onclick="toggleRecording()">
                Start Recording
            </button>
            <button onclick="playLastResponse()">Play Response</button>
        </div>
        
        <div class="transcript">
            <h3>Transcript:</h3>
            <div id="transcript">Waiting for speech...</div>
        </div>
        
        <div class="response">
            <h3>AI Response:</h3>
            <div id="response">Waiting for response...</div>
        </div>
        
        <div class="debug-link">
            <a href="/debug-console" target="_blank">Open Debug Console â†’</a>
        </div>
    </div>
    
    <script>
        let mediaRecorder;
        let audioChunks = [];
        let ws;
        let isRecording = false;
        let audioContext;
        let analyser;
        let microphone;
        let javascriptNode;
        let lastAudioResponse;

        async function initAudio() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Setup audio analysis for visualization
                audioContext = new AudioContext();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);
                
                analyser.smoothingTimeConstant = 0.8;
                analyser.fftSize = 1024;
                
                microphone.connect(analyser);
                analyser.connect(javascriptNode);
                javascriptNode.connect(audioContext.destination);
                
                javascriptNode.onaudioprocess = () => {
                    if (isRecording) {
                        const array = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(array);
                        const average = array.reduce((a, b) => a + b) / array.length;
                        document.getElementById('audioLevel').style.width = (average / 256 * 100) + '%';
                    }
                };
                
                // Setup MediaRecorder
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    audioChunks = [];
                    await sendAudioToServer(audioBlob);
                };
                
                return true;
            } catch (err) {
                console.error('Error accessing microphone:', err);
                document.getElementById('status').textContent = 'Error: Cannot access microphone';
                return false;
            }
        }

        async function toggleRecording() {
            const btn = document.getElementById('recordBtn');
            
            if (!isRecording) {
                // Initialize audio if not already done
                if (!mediaRecorder) {
                    const initialized = await initAudio();
                    if (!initialized) return;
                }
                
                // Connect to WebSocket
                connectWebSocket();
                
                // Start recording
                mediaRecorder.start();
                isRecording = true;
                btn.textContent = 'Stop Recording';
                btn.classList.add('recording');
                document.getElementById('status').textContent = 'Recording...';
                document.getElementById('transcript').textContent = 'Listening...';
            } else {
                // Stop recording
                mediaRecorder.stop();
                isRecording = false;
                btn.textContent = 'Start Recording';
                btn.classList.remove('recording');
                document.getElementById('status').textContent = 'Processing...';
                document.getElementById('audioLevel').style.width = '0%';
            }
        }

        function connectWebSocket() {
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            ws = new WebSocket(`${protocol}//${window.location.host}/ws/audio`);
            
            ws.onopen = () => {
                console.log('WebSocket connected');
            };
            
            ws.onmessage = (event) => {
                if (event.data instanceof Blob) {
                    // Audio response
                    lastAudioResponse = event.data;
                    playAudioResponse(event.data);
                } else {
                    // Text response
                    const data = JSON.parse(event.data);
                    if (data.transcript) {
                        document.getElementById('transcript').textContent = data.transcript;
                    }
                    if (data.response) {
                        document.getElementById('response').textContent = data.response;
                    }
                }
            };
            
            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                document.getElementById('status').textContent = 'Connection error';
            };
            
            ws.onclose = () => {
                console.log('WebSocket disconnected');
            };
        }

        async function sendAudioToServer(audioBlob) {
            if (ws && ws.readyState === WebSocket.OPEN) {
                // Convert blob to array buffer
                const arrayBuffer = await audioBlob.arrayBuffer();
                ws.send(arrayBuffer);
                document.getElementById('status').textContent = 'Audio sent, waiting for response...';
            } else {
                document.getElementById('status').textContent = 'Not connected to server';
            }
            
            // Close WebSocket after sending
            setTimeout(() => {
                if (ws) ws.close();
            }, 1000);
        }

        async function playAudioResponse(audioBlob) {
            const audioUrl = URL.createObjectURL(audioBlob);
            const audio = new Audio(audioUrl);
            audio.play();
            document.getElementById('status').textContent = 'Playing response...';
            
            audio.onended = () => {
                document.getElementById('status').textContent = 'Ready';
                URL.revokeObjectURL(audioUrl);
            };
        }

        function playLastResponse() {
            if (lastAudioResponse) {
                playAudioResponse(lastAudioResponse);
            } else {
                document.getElementById('status').textContent = 'No response to play';
            }
        }

        // Request microphone permission on load
        window.addEventListener('load', () => {
            // Check if browser supports required APIs
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                document.getElementById('status').textContent = 
                    'Your browser does not support audio recording';
                document.getElementById('recordBtn').disabled = true;
            }
        });
    </script>
</body>
</html>